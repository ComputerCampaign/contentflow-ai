#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import os
import sys
import json
import requests
from urllib.parse import urlparse

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from crawler.utils.xpath_manager import XPathManager
from crawler.core import CrawlerCore
from crawler.config import crawler_config
from crawler.logger import setup_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logger('crawler.crawler', file_path=__file__)

# ç®€åŒ–çš„é€šçŸ¥å™¨ç±»
class SimpleNotifier:
    def __init__(self):
        self.enabled = False
    
    def send_notification(self, *args, **kwargs):
        pass

notifier = SimpleNotifier()

def _update_task_status_via_api(task_id, status, result=None, error_message=None):
    """è°ƒç”¨åç«¯æ¥å£æ›´æ–°ä»»åŠ¡çŠ¶æ€
    
    Args:
        task_id (str): ä»»åŠ¡ID
        status (str): ä»»åŠ¡çŠ¶æ€ ('success', 'failed')
        result (str, optional): ä»»åŠ¡ç»“æœ
        error_message (str, optional): é”™è¯¯ä¿¡æ¯
    """
    try:
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å–åç«¯APIåœ°å€
        api_base_url = os.getenv('BACKEND_API_URL', 'http://localhost:5002/api/v1')
        url = f"{api_base_url}/tasks/{task_id}/status-airflow"
        
        payload = {'status': status}
        if result:
            payload['result'] = result
        if error_message:
            payload['error_message'] = error_message
        
        headers = {
            'Content-Type': 'application/json',
            'X-API-Key': os.getenv('AIRFLOW_API_KEY', 'airflow-secret-key')
        }
        
        logger.info(f"ğŸŒ [CRAWLER] è°ƒç”¨åç«¯æ¥å£æ›´æ–°ä»»åŠ¡çŠ¶æ€")
        logger.info(f"   - URL: {url}")
        logger.info(f"   - Payload: {payload}")
        logger.info(f"   - API Key: {headers['X-API-Key'][:10]}...")
        
        response = requests.put(url, json=payload, headers=headers, timeout=30)
        
        if response.status_code == 200:
            logger.info(f"âœ… [CRAWLER] ä»»åŠ¡ {task_id} çŠ¶æ€å·²æˆåŠŸæ›´æ–°ä¸º: {status}")
            try:
                response_data = response.json()
                logger.info(f"   - åç«¯å“åº”: {response_data}")
            except:
                logger.info(f"   - åç«¯å“åº”: {response.text}")
        else:
            logger.error(f"âŒ [CRAWLER] æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: HTTP {response.status_code}")
            logger.error(f"   - å“åº”å†…å®¹: {response.text}")
            
    except Exception as e:
        logger.error(f"ğŸ’¥ [CRAWLER] è°ƒç”¨åç«¯æ¥å£æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“çˆ¬è™«ä¸»æµç¨‹

def _upload_crawler_results(task_dir, task_id=None):
    """ä¸Šä¼ çˆ¬è™«ç»“æœåˆ°åç«¯æ•°æ®åº“
    
    Args:
        task_dir (str): ä»»åŠ¡ç›®å½•è·¯å¾„
        task_id (str, optional): ä»»åŠ¡ID
    """
    try:
        if not task_dir or not os.path.exists(task_dir):
            logger.error(f"âŒ [CRAWLER] ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨: {task_dir}")
            return False
            
        # æŸ¥æ‰¾metadata.jsonæ–‡ä»¶
        metadata_path = os.path.join(task_dir, 'metadata', 'metadata.json')
        if not os.path.exists(metadata_path):
            logger.error(f"âŒ [CRAWLER] metadata.jsonæ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
            return False
            
        # è¯»å–metadataæ–‡ä»¶
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
            
        logger.info(f"ğŸ“„ [CRAWLER] è¯»å–metadataæ–‡ä»¶æˆåŠŸ: {metadata_path}")
        
        # è·å–task_idï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„task_idï¼Œå…¶æ¬¡ä½¿ç”¨metadataä¸­çš„task_id
        actual_task_id = task_id or metadata.get('task_id')
        
        # è½¬æ¢ä¸ºçˆ¬è™«ç»“æœæ ¼å¼
        crawler_result = {
            'task_id': actual_task_id,  # åœ¨é¡¶å±‚è®¾ç½®task_idä½œä¸ºä¸»è¦æ ‡è¯†ç¬¦
            'url': metadata.get('url', ''),
            'title': metadata.get('title', ''),
            'content': metadata.get('description', ''),
            'extracted_data': {
                'images': metadata.get('images', []),
                'texts': metadata.get('texts', []),
                'links': metadata.get('links', []),
                'comments': metadata.get('comments', []),
                'comments_count': metadata.get('comments_count', 0),
                'xpath_rules_used': metadata.get('xpath_rules_used', []),
                'task_id': actual_task_id  # åœ¨extracted_dataä¸­ä¹Ÿä¿ç•™task_id
            },
            'page_metadata': {
                'crawl_time': metadata.get('crawl_time'),
                'task_name': metadata.get('task_name'),
                'download_result': metadata.get('download_result', [])
            },
            'status': 'success',
            'error_message': None,
            'response_code': 200,
            'response_time': 1.5,
            'content_type': 'text/html',
            'content_length': len(str(metadata)),
            'processing_time': 2.0,
            'retry_count': 0,
            'images': metadata.get('images', []),
            'files': metadata.get('download_result', [])
        }
        
        payload = {
            'results': [crawler_result],
        }
            
        # ä¸Šä¼ åˆ°åç«¯API
        url = f"{os.getenv('BACKEND_API_URL', 'http://localhost:5002/api/v1')}/tasks/crawler/results/upload"
        headers = {
            'Content-Type': 'application/json',
            'X-API-Key': os.getenv('AIRFLOW_API_KEY', 'airflow-secret-key')
        }
        
        logger.info(f"ğŸŒ [CRAWLER] ä¸Šä¼ çˆ¬è™«ç»“æœåˆ°åç«¯API")
        logger.info(f"   - URL: {url}")
        logger.info(f"   - æ•°æ®æ¡æ•°: {len(payload['results'])}")
        logger.info(f"   - API Key: {headers['X-API-Key'][:10]}...")
        
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        
        if response.status_code == 201:
            logger.info(f"âœ… [CRAWLER] çˆ¬è™«ç»“æœä¸Šä¼ æˆåŠŸ")
            try:
                response_data = response.json()
                logger.info(f"   - ä¸Šä¼ æ¡æ•°: {response_data.get('data', {}).get('uploaded_count', 0)}")
                logger.info(f"   - æˆåŠŸæ¡æ•°: {response_data.get('data', {}).get('success_count', 0)}")
                logger.info(f"   - å¤±è´¥æ¡æ•°: {response_data.get('data', {}).get('failed_count', 0)}")
                return True
            except:
                logger.info(f"   - åç«¯å“åº”: {response.text}")
                return True
        else:
            logger.error(f"âŒ [CRAWLER] ä¸Šä¼ çˆ¬è™«ç»“æœå¤±è´¥: HTTP {response.status_code}")
            logger.error(f"   - å“åº”å†…å®¹: {response.text}")
            return False
            
    except Exception as e:
        logger.error(f"ğŸ’¥ [CRAWLER] ä¸Šä¼ çˆ¬è™«ç»“æœå¤±è´¥: {str(e)}")
        return False

class Crawler:
    """ç½‘é¡µçˆ¬è™«ï¼Œç”¨äºæŠ“å–å›¾ç‰‡å’Œæ ‡é¢˜ä¿¡æ¯"""
    
    def __init__(self, output_dir=None, data_dir=None, timeout=None, retry=None, use_selenium=None, enable_xpath=None, max_workers=None):
        """åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            output_dir (str, optional): è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¸´æ—¶æ–‡ä»¶å’Œæ—¥å¿—ï¼‰
            data_dir (str, optional): æ•°æ®å­˜å‚¨ç›®å½•ï¼ˆç”¨äºä¿å­˜å›¾ç‰‡å’Œå…ƒæ•°æ®ï¼‰
            timeout (int, optional): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            retry (int, optional): å¤±è´¥é‡è¯•æ¬¡æ•°
            use_selenium (bool, optional): æ˜¯å¦ä½¿ç”¨Selenium
            enable_xpath (bool, optional): æ˜¯å¦å¯ç”¨XPathé€‰æ‹©å™¨
            max_workers (int, optional): æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
        """
        # åˆå§‹åŒ–çˆ¬è™«æ ¸å¿ƒ
        self.crawler_core = CrawlerCore(
            output_dir=output_dir,
            data_dir=data_dir,
            timeout=timeout,
            retry=retry,
            use_selenium=use_selenium,
            enable_xpath=enable_xpath,
            max_workers=max_workers
        )
        
        logger.info(f"çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.crawler_core.output_dir}, æ•°æ®ç›®å½•: {self.crawler_core.data_dir}")
        logger.info(f"é‚®ä»¶é€šçŸ¥: {'å·²å¯ç”¨' if notifier.enabled else 'æœªå¯ç”¨'}")
    
    def crawl(self, url, rule_ids=None, task_name=None, task_id=None):
        """çˆ¬å–æŒ‡å®šURLçš„å›¾ç‰‡å’Œæ ‡é¢˜
        
        Args:
            url (str): è¦çˆ¬å–çš„URL
            rule_ids (list, optional): XPathè§„åˆ™IDåˆ—è¡¨ï¼Œç”¨äºæŒ‡å®šä½¿ç”¨å“ªäº›XPathè§„åˆ™
            task_name (str, optional): ä»»åŠ¡åç§°ï¼Œç”¨ä½œæ•°æ®å­˜å‚¨çš„æ–‡ä»¶å¤¹åç§°
            task_id (str, optional): ä»»åŠ¡IDï¼Œç”¨äºå­˜å‚¨åˆ°å…ƒæ•°æ®ä¸­
            
        Returns:
            tuple: (æ˜¯å¦æˆåŠŸ, ä»»åŠ¡åç§°, ä»»åŠ¡ç›®å½•)
        """
        result = self.crawler_core.crawl_url(url, task_name, rule_ids=rule_ids, task_id=task_id)
        if result.get('success'):
            return True, result.get('task_name'), result.get('task_dir')
        else:
            return False, None, None
    
    def close(self):
        """å…³é—­èµ„æº"""
        # CrawlerCoreæ²¡æœ‰closeæ–¹æ³•ï¼Œè¿™é‡Œåªæ˜¯å ä½
        pass

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="ç½‘é¡µå›¾ç‰‡å’Œæ ‡é¢˜çˆ¬è™«")
    parser.add_argument("--url", help="è¦çˆ¬å–çš„ç½‘é¡µURL")
    parser.add_argument("--task-id", help="ä»»åŠ¡IDï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ")
    parser.add_argument("--task-name", help="ä»»åŠ¡åç§°ï¼Œç”¨ä½œæ•°æ®å­˜å‚¨çš„æ–‡ä»¶å¤¹åç§°")
    parser.add_argument("--output", help="è¾“å‡ºç›®å½•ï¼Œç”¨äºä¸´æ—¶æ–‡ä»¶å’Œæ—¥å¿—ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
    parser.add_argument("--data-dir", help="æ•°æ®å­˜å‚¨ç›®å½•ï¼Œç”¨äºä¿å­˜å›¾ç‰‡å’Œå…ƒæ•°æ®ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
    parser.add_argument("--use-selenium", type=lambda x: x.lower() == 'true', help="ä½¿ç”¨Seleniumå’ŒChromeDriverè¿›è¡Œçˆ¬å–ï¼Œå€¼ä¸ºtrueæˆ–false")
    parser.add_argument("--timeout", type=int, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œå•ä½ä¸ºç§’ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
    parser.add_argument("--retry", type=int, help="å¤±è´¥é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¸º'config.json'ï¼‰")
    parser.add_argument("--email-notification", type=lambda x: x.lower() == 'true',help="æ˜¯å¦å¯ç”¨é‚®ä»¶é€šçŸ¥ï¼Œå€¼ä¸ºtrueæˆ–false")
    # Selenium é…ç½®å‚æ•°
    parser.add_argument("--headless", type=lambda x: x.lower() == 'true', help="Seleniumæ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢ï¼‰ï¼Œå€¼ä¸ºtrueæˆ–false")
    parser.add_argument("--proxy", help="Seleniumä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨åœ°å€ï¼Œæ ¼å¼ä¸ºhttp://host:portæˆ–socks5://host:port")
    parser.add_argument("--page-load-wait", type=int, help="Seleniumé¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´ï¼Œå•ä½ä¸ºç§’")
    parser.add_argument("--user-agent", help="Seleniumä½¿ç”¨çš„ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²")
    parser.add_argument("--rule-ids", help="XPathè§„åˆ™IDåˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼Œç”¨äºæŒ‡å®šä½¿ç”¨å“ªäº›XPathè§„åˆ™ï¼ˆä¾‹å¦‚ï¼šreddit_media,reddit_commentsï¼‰")
    parser.add_argument("--enable-xpath", type=lambda x: x.lower() == 'true', help="å¯ç”¨XPathé€‰æ‹©å™¨ï¼Œä½¿ç”¨XPathè§„åˆ™è§£æé¡µé¢ï¼Œå€¼ä¸ºtrueæˆ–falseï¼ˆé»˜è®¤å€¼å–å†³äºconfig.jsonä¸­xpath_config.enabledçš„å€¼ï¼‰")
    parser.add_argument("--list-rules", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„XPathè§„åˆ™")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    if args.config:
        # ç®€åŒ–é…ç½®å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨crawler_config
        pass
    
    # å¤„ç†é‚®ä»¶é€šçŸ¥
    if args.email_notification is not None:
        # ç®€åŒ–å¤„ç†ï¼Œcrawleræ¨¡å—æš‚ä¸æ”¯æŒé‚®ä»¶é€šçŸ¥
        pass
    
    # å¤„ç† Selenium é…ç½®å‚æ•°
    if args.headless is not None:
        crawler_config.selenium_headless = args.headless
    if args.proxy is not None:
        crawler_config.selenium_proxy = args.proxy
    if args.page_load_wait is not None:
        crawler_config.selenium_page_load_wait = args.page_load_wait
    if args.user_agent is not None:
        crawler_config.selenium_user_agent = args.user_agent
    
    # å¦‚æœæŒ‡å®šäº†åˆ—å‡ºè§„åˆ™
    if args.list_rules:
        # ä»æ­£ç¡®çš„æ¨¡å—å¯¼å…¥XPathManager
        from crawler.utils.xpath_manager import XPathManager
        # åˆ›å»ºXPathManagerå®ä¾‹å¹¶è°ƒç”¨list_rulesæ–¹æ³•
        xpath_manager = XPathManager()
        print(xpath_manager.list_rules())
        sys.exit(0)
    
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†URL
    if not args.url:
        logger.error("æœªæä¾›URLï¼Œè¯·ä½¿ç”¨--urlå‚æ•°æŒ‡å®šè¦çˆ¬å–çš„ç½‘é¡µURL")
        parser.print_help()
        sys.exit(1)
        
    # éªŒè¯URL
    try:
        result = urlparse(args.url)
        if not all([result.scheme, result.netloc]):
            logger.error(f"æ— æ•ˆçš„URL: {args.url}")
            sys.exit(1)
    except Exception:
        logger.error(f"æ— æ•ˆçš„URL: {args.url}")
        sys.exit(1)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = Crawler(
        output_dir=args.output,
        data_dir=args.data_dir,
        timeout=args.timeout,
        retry=args.retry,
        use_selenium=args.use_selenium,
        enable_xpath=args.enable_xpath
    )
    
    try:
        # è§£æè§„åˆ™IDåˆ—è¡¨
        rule_ids = None
        if args.rule_ids:
            rule_ids = [rule_id.strip() for rule_id in args.rule_ids.split(',') if rule_id.strip()]
            logger.info(f"æŒ‡å®šçš„XPathè§„åˆ™ID: {rule_ids}")
        
        # å¼€å§‹çˆ¬å–ï¼Œä¼ å…¥è§„åˆ™IDåˆ—è¡¨ã€ä»»åŠ¡IDå’Œä»»åŠ¡åç§°
        task_name = args.task_name if args.task_name else args.task_id
        success, task_id, task_dir = crawler.crawl(args.url, rule_ids, task_name, args.task_id)
        
        # æ‰“å°çˆ¬å–ç»“æœä¿¡æ¯
        logger.info(f"ğŸ¯ [CRAWLER] çˆ¬å–ç»“æœ - æˆåŠŸ: {success}, ä»»åŠ¡ID: {task_id}, ä»»åŠ¡ç›®å½•: {task_dir}")
        
        # å¦‚æœçˆ¬å–æˆåŠŸï¼Œä¸Šä¼ çˆ¬è™«ç»“æœåˆ°æ•°æ®åº“
        if success and task_dir:
            logger.info(f"ğŸ“¤ [CRAWLER] å‡†å¤‡ä¸Šä¼ çˆ¬è™«ç»“æœåˆ°æ•°æ®åº“")
            upload_success = _upload_crawler_results(task_dir, args.task_id)
            if upload_success:
                logger.info(f"âœ… [CRAWLER] çˆ¬è™«ç»“æœä¸Šä¼ æˆåŠŸ")
            else:
                logger.warning(f"âš ï¸ [CRAWLER] çˆ¬è™«ç»“æœä¸Šä¼ å¤±è´¥ï¼Œä½†ä¸å½±å“çˆ¬å–ä»»åŠ¡")
        
        # è°ƒç”¨åç«¯æ¥å£æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if args.task_id:
            logger.info(f"ğŸ“¡ [CRAWLER] å‡†å¤‡è°ƒç”¨åç«¯æ¥å£æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œä»»åŠ¡ID: {args.task_id}")
            _update_task_status_via_api(args.task_id, 'completed' if success else 'failed', task_dir if success else None)
        else:
            logger.warning(f"âš ï¸ [CRAWLER] æœªæä¾›ä»»åŠ¡IDï¼Œè·³è¿‡çŠ¶æ€å›ä¼ ")
        
        if not success:
            sys.exit(1)
        logger.info(f"çˆ¬å–å®Œæˆï¼Œä»»åŠ¡ID: {task_id}, ä»»åŠ¡ç›®å½•: {task_dir}")
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
        if args.task_id:
            _update_task_status_via_api(args.task_id, 'failed', None, 'ç”¨æˆ·ä¸­æ–­')
    except Exception as e:
        logger.exception(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        if args.task_id:
            _update_task_status_via_api(args.task_id, 'failed', None, str(e))
        sys.exit(1)
    finally:
        # å…³é—­èµ„æº
        crawler.close()

if __name__ == "__main__":
    main()